{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HtI6H6dxcvEn"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dC1ewu-RcvEs"
      },
      "source": [
        "<table align=\"left\">\n",
        "    <td>\n",
        "        <a href=\"https://console.cloud.google.com/mlengine/notebooks/deploy-notebook?q=download_url%3Dhttps://github.com/tensorflow/cloud/blob/master/examples/cloud_fit.ipynb\">\n",
        "            <img src=\"https://www.gstatic.com/images/branding/product/1x/google_cloud_48dp.png\" alt=\"AI Platform Notebooks\"> Run in AI Platform Notebooks\n",
        "        </a>\n",
        "    </td>\n",
        "    <td>\n",
        "        <a href=\"https://colab.research.google.com/github/tensorflow/cloud/blob/master/examples/cloud_fit.ipynb\">\n",
        "            <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "    </td>\n",
        "    <td>\n",
        "        <a href=\"https://github.com/tensorflow/cloud/blob/master/examples/cloud_fit.ipynb\">\n",
        "            <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">View on GitHub\n",
        "        </a>\n",
        "     </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R2-BJ_QbcvEs"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Following is a quick introduction to `cloud_fit`. `cloud_fit` enables training on Google Cloud AI Platform in the same manner as `model.fit()`.\n",
        "In this notebook, we will start by installing libraries required, then proceed with two samples showing how to use `numpy.array` and `tf.data.dataset` with `cloud_fit`\n",
        "\n",
        "\n",
        "### What are the components of `cloud_fit()`?\n",
        "\n",
        "`cloud_fit` has two main components as follows:\n",
        "\n",
        "**client.py:** serializes the provided data and model along with typical `model.fit()` parameters and triggers a AI platform training\n",
        "``` python\n",
        "def cloud_fit(model,\n",
        "              remote_dir: Text,\n",
        "              region: Text = None,\n",
        "              project_id: Text = None,\n",
        "              image_uri: Text = None,\n",
        "              distribution_strategy: Text = DEFAULT_DISTRIBUTION_STRATEGY,\n",
        "              job_spec: Dict[str, Any] = None,\n",
        "              job_id: Text = None,\n",
        "              **fit_kwargs) -> Text:\n",
        "  \"\"\"Facilitates remote execution of in memory Models and Datasets on AI Platform.\n",
        "\n",
        "  Args:\n",
        "    model: A compiled Keras Model.\n",
        "    remote_dir: Google Cloud Storage path for temporary assets and AI Platform\n",
        "      training output. Will overwrite value in job_spec.\n",
        "    region: Target region for running the AI Platform Training job.\n",
        "    project_id: Project id where the training should be deployed to.\n",
        "    image_uri: base image used to use for AI Platform Training\n",
        "    distribution_strategy: Specifies the distribution strategy for remote\n",
        "      execution when a jobspec is provided. Accepted values are strategy names\n",
        "      as specified by 'tf.distribute.<strategy>.__name__'.\n",
        "    job_spec: AI Platform training job_spec, will take precedence over all other\n",
        "      provided values except for remote_dir. If none is provided a default\n",
        "      cluster spec and distribution strategy will be used.\n",
        "    job_id: A name to use for the AI Platform Training job (mixed-case letters,\n",
        "      numbers, and underscores only, starting with a letter).\n",
        "    **fit_kwargs: Args to pass to model.fit() including training and eval data.\n",
        "      Only keyword arguments are supported. Callback functions will be\n",
        "      serialized as is.\n",
        "\n",
        "  Returns:\n",
        "    AI Platform job ID\n",
        "\n",
        "  Raises:\n",
        "    RuntimeError: If executing in graph mode, eager execution is required for\n",
        "    cloud_fit.\n",
        "    NotImplementedError: Tensorflow v1.x is not supported.\n",
        "  \"\"\"\n",
        "```\n",
        "\n",
        "**remote.py:** A job that takes in a remote_dir as parameter , load model and data from this location and executes the training with stored parameters.\n",
        "```python\n",
        "def run(remote_dir: Text, distribution_strategy_text: Text):\n",
        "  \"\"\"deserializes Model and Dataset and runs them.\n",
        "\n",
        "  Args:\n",
        "    remote_dir: Temporary cloud storage folder that contains model and Dataset\n",
        "      graph. This folder is also used for job output.\n",
        "    distribution_strategy_text: Specifies the distribution strategy for remote\n",
        "      execution when a jobspec is provided. Accepted values are strategy names\n",
        "      as specified by 'tf.distribute.<strategy>.__name__'.\n",
        "  \"\"\"\n",
        "```\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* AI Platform Training\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [AI Platform Training\n",
        "pricing](https://cloud.google.com/ai-platform/training/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QwMvZmuucvEt"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project.](https://console.cloud.google.com/cloud-resource-manager) When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the AI Platform APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com)\n",
        "\n",
        "4. If running locally on your own machine, you will need to install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BxnMQKyDcvEt"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using [AI Platform Notebooks](https://cloud.google.com/ai-platform/notebooks/docs/)**, your environment is already\n",
        "authenticated. Skip these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8ztCXRy5cvEu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your Google Cloud account. This provides access\n",
        "# to your Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this tutorial in a notebook locally, replace the string\n",
        "# below with the path to your service account key and run this cell to\n",
        "# authenticate your Google Cloud account.\n",
        "else:\n",
        "    %env GOOGLE_APPLICATION_CREDENTIALS your_path_to_credentials.json\n",
        "\n",
        "# Log in to your account on Google Cloud\n",
        "! gcloud auth application-default login --quiet\n",
        "! gcloud auth login --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z8GvrsnmcvEx"
      },
      "source": [
        "## Clone and build tensorflow_cloud\n",
        "\n",
        "To use the latest version of the tensorflow_cloud, we will clone and build the repo. The resulting whl file is both used in the client side as well as in construction of a docker image for remote execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O4gedCSZcvEy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tensorflow/cloud.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zafJjYrXcvE1"
      },
      "outputs": [],
      "source": [
        "!cd cloud/src/python && python3 setup.py -q bdist_wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tPOgkeSncvE4"
      },
      "outputs": [],
      "source": [
        "!pip install -U cloud/src/python/dist/tensorflow_cloud-*.whl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyF5aOg0cvE6"
      },
      "source": [
        "#### Restart the Kernel\n",
        "\n",
        "We will automatically restart your kernel so the notebook has access to the packages you installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ToQrN8-LcvE7"
      },
      "outputs": [],
      "source": [
        "# Restart the kernel after pip installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "guLbIl38cvE9"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wDz9fPz8cvE-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_cloud.tuner import cloud_fit_client as client\n",
        "\n",
        "# Setup and imports\n",
        "REMOTE_DIR = '[gcs-bucket-for-temporary-files]' #@param {type:\"string\"}\n",
        "REGION = 'us-central1' #@param {type:\"string\"}\n",
        "PROJECT_ID = '[your-project-id]' #@param {type:\"string\"}\n",
        "DOCKER_IMAGE_NAME = '[name-for-docker-image]' #@param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{DOCKER_IMAGE_NAME}:latest' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BTkvsih7cvFA"
      },
      "source": [
        "### Create a docker file with tensorflow_cloud\n",
        "In the next step we create a base docker file with the latest wheel file to use for remote training. You may use any base image. However, DLVM base images come pre-installed with most needed packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DYogthoYcvFB"
      },
      "outputs": [],
      "source": [
        "%%file Dockerfile\n",
        "\n",
        "# Using DLVM base image\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
        "WORKDIR /root\n",
        "\n",
        "# Path configuration\n",
        "ENV PATH $PATH:/root/tools/google-cloud-sdk/bin\n",
        "\n",
        "# Make sure gsutil will use the default service account\n",
        "RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
        "\n",
        "# Copy and install tensorflow_cloud wheel file\n",
        "ADD cloud/src/python/dist/tensorflow_cloud-*.whl /tmp/\n",
        "RUN pip3 install --upgrade /tmp/tensorflow_cloud-*.whl --quiet\n",
        "\n",
        "# Sets up the entry point to invoke cloud_fit.\n",
        "ENTRYPOINT [\"python3\",\"-m\",\"tensorflow_cloud.tuner.cloud_fit_remote\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3XRniIFxcvFE"
      },
      "outputs": [],
      "source": [
        "!docker build -t {IMAGE_URI} -f Dockerfile . -q && docker push {IMAGE_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HXzI9doLcvFH"
      },
      "source": [
        "## Tutorial 1 - Functional model\n",
        "In this sample we will demonstrate using numpy.array as input data by creating a basic model and and submit it for remote training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DNo06kD2cvFH"
      },
      "source": [
        "### Define model building function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tOnLcZPHcvFH"
      },
      "outputs": [],
      "source": [
        "\"\"\"Simple model to compute y = wx + 1, with w trainable.\"\"\"\n",
        "inp = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n",
        "times_w = tf.keras.layers.Dense(\n",
        "  units=1,\n",
        "  kernel_initializer=tf.keras.initializers.Constant([[0.5]]),\n",
        "  kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "  use_bias=False)\n",
        "plus_1 = tf.keras.layers.Dense(\n",
        "  units=1,\n",
        "  kernel_initializer=tf.keras.initializers.Constant([[1.0]]),\n",
        "  bias_initializer=tf.keras.initializers.Constant([1.0]),\n",
        "  trainable=False)\n",
        "outp = plus_1(times_w(inp))\n",
        "simple_model = tf.keras.Model(inp, outp)\n",
        "\n",
        "simple_model.compile(tf.keras.optimizers.SGD(0.002),\n",
        "              \"mean_squared_error\", run_eagerly=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IBqtbBoqcvFK"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UfE9TjpJcvFK"
      },
      "outputs": [],
      "source": [
        "# Creating sample data\n",
        "x = [[9.], [10.], [11.]] * 10\n",
        "y = [[xi[0]/2. + 6] for xi in x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X0KTGKQWcvFN"
      },
      "source": [
        "### Run the model locally for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xPlTCi4EcvFN"
      },
      "outputs": [],
      "source": [
        "# Verify the model by training locally for one step.\n",
        "simple_model.fit(np.array(x), np.array(y), batch_size=len(x), epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XnGy5_8EcvFP"
      },
      "source": [
        "### Submit model and dataset for remote training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WBL9D0LpcvFP"
      },
      "outputs": [],
      "source": [
        "# Create a unique remote sub folder path for assets and model training output.\n",
        "SIMPLE_REMOTE_DIR = os.path.join(REMOTE_DIR, str(uuid.uuid4()))\n",
        "print('your remote folder is %s' % (SIMPLE_REMOTE_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n4vsNr1UcvFS"
      },
      "outputs": [],
      "source": [
        "# Using default configuration with two workers dividing the dataset between the two.\n",
        "simple_model_job_id = client.cloud_fit(model=simple_model, remote_dir = SIMPLE_REMOTE_DIR, region =REGION , image_uri=IMAGE_URI, x=np.array(x), y=np.array(y), epochs=100, steps_per_epoch=len(x)/2,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DwvY8GkScvFV"
      },
      "outputs": [],
      "source": [
        "!gcloud ai-platform jobs describe projects/{PROJECT_ID}/jobs/{simple_model_job_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zdkaGQxncvFY"
      },
      "source": [
        "### Retrieve the trained model\n",
        "Once the training is complete you can access the trained model at `remote_folder/output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R3GwKmHscvFY"
      },
      "outputs": [],
      "source": [
        "# Load the trained model from gcs bucket\n",
        "trained_simple_model = tf.keras.models.load_model(os.path.join(SIMPLE_REMOTE_DIR, 'output'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C3tur-zBcvFa"
      },
      "outputs": [],
      "source": [
        "# Test that the saved model loads and works properly\n",
        "trained_simple_model.evaluate(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TLMKhotIcvFc"
      },
      "source": [
        "## Tutorial 2 - Sequential Models and Datasets\n",
        "In this sample we will demonstrate using datasets by creating a basic model and submitting it for remote training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ierzYSPgcvFd"
      },
      "source": [
        "### Define model building function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hds9GY-wcvFd"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "fashion_mnist_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "fashion_mnist_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YL7BFIN1cvFf"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uJB2taSscvFg"
      },
      "outputs": [],
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "images, labels = train\n",
        "images = images/255\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset = dataset.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x2vNQZtBcvFj"
      },
      "source": [
        "### Run the model locally for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nMJGqAAWcvFk"
      },
      "outputs": [],
      "source": [
        "# Verify the model by training locally for one step. This is not necessary prior to cloud.fit() however it is recommended.\n",
        "fashion_mnist_model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9akpewgocvFm"
      },
      "source": [
        "### Submit model and dataset for remote training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XYFj54K1cvFm"
      },
      "outputs": [],
      "source": [
        "# Create a unique remote sub folder path for assets and model training output.\n",
        "FASHION_REMOTE_DIR = os.path.join(REMOTE_DIR, str(uuid.uuid4()))\n",
        "print('your remote folder is %s' % (FASHION_REMOTE_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d25en9VKcvFo"
      },
      "outputs": [],
      "source": [
        "fashion_mnist_model_job_id = client.cloud_fit(model=fashion_mnist_model, remote_dir = FASHION_REMOTE_DIR,region =REGION , image_uri=IMAGE_URI,  x=dataset,epochs=10, steps_per_epoch=15,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4zN6akYkcvFq"
      },
      "outputs": [],
      "source": [
        "!gcloud ai-platform jobs describe projects/{PROJECT_ID}/jobs/{fashion_mnist_model_job_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4I9zkkCkcvFt"
      },
      "source": [
        "### Retrieve the trained model\n",
        "Once the training is complete you can access the trained model at remote_folder/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "unfe2f5NcvFt"
      },
      "outputs": [],
      "source": [
        "# Load the trained model from gcs bucket\n",
        "trained_fashion_mnist_model = tf.keras.models.load_model(os.path.join(FASHION_REMOTE_DIR, 'output'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8gscKRmTcvFv"
      },
      "outputs": [],
      "source": [
        "# Test that the saved model loads and works properly\n",
        "test_images, test_labels = test\n",
        "test_images = test_images/255\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_dataset = test_dataset.batch(32)\n",
        "\n",
        "trained_fashion_mnist_model.evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cloud_fit.ipynb",
      "provenance": [
        {
          "file_id": "1135EiCC0CKFhSUX5aM38sTFfDxKAmvvo",
          "timestamp": 1593474025992
        }
      ]
    },
    "environment": {
      "name": "tf2-2-2-gpu.2-2.m49",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}